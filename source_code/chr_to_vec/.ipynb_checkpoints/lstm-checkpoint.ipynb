{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "#import tensorflow as tf\n",
    "\n",
    "import io\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from io import StringIO\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import types\n",
    "from IPython import get_ipython #for import notebook\n",
    "from nbformat import read #for import notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell #for import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_notebook(fullname, path=None): #for import notebook\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotebookLoader(object): #for import notebook\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from pre_process.ipynb\n",
      "\n",
      " [[1561, 1, 1089, 1, 102, 33, 74, 14, 7, 229, 3510, 218, 354, 4], [32, 39, 16, 10, 4904, 43, 10, 68, 44, 60, 14, 81, 2579, 0], [29, 21, 3, 225, 4, 41, 35, 93, 149, 6, 1314, 0], [71, 3, 68, 38, 40, 4, 2, 64, 0, 41, 35, 51, 106, 149, 1473, 8, 2582, 1735, 0, 1409, 177, 61, 4], [2, 325, 3, 19, 6800, 57, 654, 30, 21, 4, 2, 64, 207, 24, 1492, 36, 224, 0], [2, 746, 7, 547, 144, 6, 5, 1215, 230, 30, 23, 348, 4963, 8, 273, 56, 12, 81, 358, 0], [97, 7, 60, 228, 0, 2, 318, 476, 8, 2153, 364, 47, 46, 6, 348, 'pingpong.Perhaps', 30, 23, 106, 7, 'foursome', 28, 99, 0], [609, 133, 6, 20, 17, 140, 88, 19, 1180, 1, 30, 121, 250, 99, 6, 47, 1383, 28, 'us.That', 10, 771, 645, 8, 449, 1, 78, 0], ['Good.Let', 161, 34, 47, 92, 0], [219, 55, 0], []]\n",
      "\n",
      " [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1], [1, 1, 1], []]\n",
      "\n",
      " 0 0 0 0 0 0 4 4 4 4\n",
      "\n",
      " 3 4 2 2 2 3 4 1 3 4\n",
      "\n",
      " [[108, 3, 21, 5877, 4], [263, 160, 2, 23, 0, 72, 7, 953, 12, 1108, 17, 2262, 9, 85, 44, 1, 2, 23, 21, 908, 5877, 7, 498, 0], [287, 4, 2, 38, 201, 2176, 17], [32, 225, 908, 5877, 4], [165, 17], [72, 394, 0, 140, 3, 21, 645, 1595, 1, 3, 23, 106, 9, 1, 78, 0], []]\n",
      "\n",
      " [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], []]\n",
      "\n",
      " 0 0 6 0 0 0\n",
      "\n",
      " 2 1 2 2 1 1\n",
      "\n",
      " [[108, 3, 644, 28, 5, 1621, 25, 4], [65, 1, 2, 893, 6, 1697, 453, 0], [29, 10, 5, 1005, 4], [63, 1621, 115, 78, 157, 'comerials', 0], [97, 419, 1, 43, 119, 3, 15, 6, 196, 7, 1263, 1275, 0], []]\n",
      "\n",
      " [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], []]\n",
      "\n",
      " 0 0 0 0 0\n",
      "\n",
      " 2 1 2 1 1\n",
      "\n",
      " [[137, 3, 50, 55, 4], [2, 35, 22, 50, 55, 295, 0, 2, 48, 5380, 113, 2, 1746, 99, 1299, 84, 5, 4763, 0], [269, 'worry.He', 10, 80, 'acrobat', 3056], [2, 54, 0], []]\n",
      "\n",
      " [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 1, 0, 1], [1, 1, 1], []]\n",
      "\n",
      " 0 0 0 0\n",
      "\n",
      " 2 1 1 1\n"
     ]
    }
   ],
   "source": [
    "import pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dataset = pre_process.transformed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11118"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class chr_rnn(nn.Module):\n",
    "    def __init__(self, hidden_size, bidirectional):\n",
    "        super(chr_rnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if(bidirectional == True):\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.chr_rnn = nn.GRU(100, 100, bidirectional = bidirectional)\n",
    "        \n",
    "        \n",
    "    def forward(self,char,h0):\n",
    "        _,h0 = self.chr_rnn(char,h0)\n",
    "        return h0\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.bidirectional , 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class last_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(last_net, self).__init__()\n",
    "        self.lastnet = nn.Linear(200,100)\n",
    "        \n",
    "    def forward(self,last_hidden_state):\n",
    "        last_w = self.lastnet(last_hidden_state)\n",
    "        return last_w\n",
    "  \n",
    "    \n",
    "chr_rnn_1 = chr_rnn(100, True).cuda()\n",
    "last_net_1 = last_net().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('/home/jongsu/Desktop/persona_dialogue/chr_parameter/my_character_rnn_526.pth'):\n",
    "    print(\"loading chr\")\n",
    "    chr_rnn_1.load_state_dict(torch.load('/home/jongsu/Desktop/persona_dialogue/chr_parameter/my_character_rnn_526.pth'))\n",
    "\n",
    "if os.path.isfile('/home/jongsu/Desktop/persona_dialogue/chr_parameter/my_character_linear_526.pth'):\n",
    "    print(\"loading last\")\n",
    "    last_net_1.load_state_dict(torch.load('/home/jongsu/Desktop/persona_dialogue/chr_parameter/my_character_linear_526.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wv_model_en = word2vec.Word2Vec(size=100, window=5, min_count=5, workers=4)\n",
    "wv_model_en = word2vec.Word2Vec.load('/home/jongsu/Desktop/intention_extraction/wv_parameter/dialogue_wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class makesent_gru(nn.Module):\n",
    "    def __init__(self, hidden_size, bidirectional):\n",
    "        super(makesent_gru, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if(bidirectional == True):\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.gru = nn.GRU(100, 100, bidirectional = bidirectional)\n",
    "        \n",
    "        \n",
    "    def forward(self,char,h0):\n",
    "        gru,h0 = self.gru(char,h0)\n",
    "        return h0\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.bidirectional , 1, self.hidden_size, device=device)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterian = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "class Gru(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Gru, self).__init__()\n",
    "        self.gru = nn.GRU(100,100,bidirectional = True)\n",
    "        self.inh = torch.zeros(2, 1, 100).cuda()\n",
    "        self.nn = nn.Linear(200,28)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(2, 1, 100).cuda()\n",
    "    \n",
    "    def Loss(self, output, target):\n",
    "        i = 0\n",
    "        loss = 0\n",
    "        while(i < len(output)):\n",
    "            \n",
    "            j = 0\n",
    "            while(j < len(output[i])):\n",
    "                \n",
    "                if (j == target[i]):\n",
    "                    \n",
    "                    \n",
    "                    loss = loss + 0\n",
    "                else:\n",
    "                    loss = loss + torch.log(output[i][j])\n",
    "                \n",
    "                j = j + 1\n",
    "            \n",
    "            target\n",
    "            i = i + 1\n",
    "            \n",
    "        return -loss\n",
    "            \n",
    "    \n",
    "    def forward(self, sentence):  \n",
    "        \n",
    "      \n",
    "        \n",
    "        output,hidden = self.gru(sentence,self.inh)\n",
    "        \n",
    "        output = self.nn(output.view(len(sentence),200))\n",
    "        \n",
    "        output2 = self.softmax(output)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class last_sent_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(last_sent_net, self).__init__()\n",
    "        self.lastnet = nn.Linear(200,100)\n",
    "        \n",
    "    def forward(self,last_hidden_state):\n",
    "        last_w = self.lastnet(last_hidden_state)\n",
    "        return last_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagset = torch.tensor(np.arange(28),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_1 = makesent_gru(100,True).cuda()\n",
    "last_sent_1 = last_sent_net().cuda()\n",
    "Gru1 = Gru(100).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif os.path.isfile(\\'/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/Bigru_1.pth\\'):\\n    print(\"loading crf\")\\n    Gru1.load_state_dict(torch.load(\\'/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/Bigru_1.pth\\'))\\n\\nif os.path.isfile(\\'/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/last_sent_1.pth\\'):\\n    print(\"loading last\")\\n    last_sent_1.load_state_dict(torch.load(\\'/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/last_sent_1.pth\\'))\\n    \\nif os.path.isfile(\\'/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/sent_1.pth\\'):\\n    print(\"loading last\")\\n    sent_1.load_state_dict(torch.load(\\'/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/sent_1.pth\\'))\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if os.path.isfile('/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/Bigru_1.pth'):\n",
    "    print(\"loading crf\")\n",
    "    Gru1.load_state_dict(torch.load('/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/Bigru_1.pth'))\n",
    "\n",
    "if os.path.isfile('/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/last_sent_1.pth'):\n",
    "    print(\"loading last\")\n",
    "    last_sent_1.load_state_dict(torch.load('/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/last_sent_1.pth'))\n",
    "    \n",
    "if os.path.isfile('/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/sent_1.pth'):\n",
    "    print(\"loading last\")\n",
    "    sent_1.load_state_dict(torch.load('/home/jongsu/Desktop/persona_dialogue/crf_lstm_parameter/sent_1.pth'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lens = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens[::-1][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.arange(10)\n",
    "lens + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[[ 0.7180, -0.1102,  0.6078,  ..., -1.3413,  0.6959,  1.5664],\n",
      "         [-1.8044, -0.8018, -0.8930,  ...,  0.4830, -2.0441, -0.4130],\n",
      "         [ 0.7650,  0.2513,  1.0740,  ..., -0.5877, -0.2748, -1.7118],\n",
      "         ...,\n",
      "         [ 1.2101,  0.1563, -2.5640,  ...,  0.5213, -0.4187, -0.2883],\n",
      "         [-0.0119,  0.1858,  0.0913,  ...,  0.9516, -0.7111, -0.8005],\n",
      "         [ 0.0893, -0.2594, -0.1451,  ...,  2.0239, -0.6212,  0.2858]],\n",
      "\n",
      "        [[-0.3570,  0.3399,  0.2410,  ..., -1.0136, -1.2953, -0.3620],\n",
      "         [-0.5911,  0.1988, -0.4088,  ...,  1.3073, -0.5890, -0.9550],\n",
      "         [-1.0861, -0.3116, -0.5129,  ..., -0.6419,  0.2673, -1.4841],\n",
      "         ...,\n",
      "         [-0.5738,  0.2410,  1.7730,  ..., -1.2751,  0.9195,  1.3656],\n",
      "         [ 0.4975,  0.1703, -1.1401,  ..., -0.9224,  0.1239, -0.1094],\n",
      "         [ 1.3168, -1.2167, -0.7636,  ...,  0.4232,  0.9514,  0.4807]],\n",
      "\n",
      "        [[ 1.3790,  0.9043, -1.6484,  ...,  1.7026, -0.0262,  1.0267],\n",
      "         [-1.0163, -0.5424, -1.1637,  ...,  1.0727,  1.5439,  0.8124],\n",
      "         [-0.0076,  0.3128, -0.7711,  ..., -0.8419, -0.0656, -1.3074],\n",
      "         ...,\n",
      "         [ 1.1189, -0.7141,  0.3803,  ..., -0.0927,  1.5000, -0.1187],\n",
      "         [ 0.6236, -1.6198, -0.4174,  ..., -0.2920, -0.3323,  0.4634],\n",
      "         [-0.5026, -0.5303, -1.3708,  ...,  0.8194,  0.9331, -0.2345]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3989, -1.0654, -1.6454,  ...,  0.4984, -0.2019,  1.6202],\n",
      "         [ 0.5324, -0.2029,  1.2078,  ...,  0.0061,  0.4106,  0.2402],\n",
      "         [-0.0978,  0.1814, -0.2333,  ...,  0.5853, -0.1685, -0.5856],\n",
      "         ...,\n",
      "         [-0.8007, -0.3922,  1.5521,  ..., -2.0000, -1.4902, -1.6357],\n",
      "         [-0.6245, -0.1989, -0.5238,  ..., -0.7546, -0.4549,  1.0580],\n",
      "         [ 0.1908,  0.6954, -0.6444,  ..., -0.9389,  0.2235, -0.4305]],\n",
      "\n",
      "        [[ 1.5238, -0.7899, -0.0279,  ...,  0.3550, -0.5205, -0.3891],\n",
      "         [ 1.2261, -0.8348,  0.5871,  ..., -0.8521, -0.7803, -0.2050],\n",
      "         [-0.5033, -0.8508,  0.8391,  ...,  0.1623, -0.5142, -0.6648],\n",
      "         ...,\n",
      "         [ 0.3570,  0.0906,  0.6703,  ..., -0.2930,  0.6898, -0.3325],\n",
      "         [ 0.2073, -0.6647, -0.8216,  ..., -0.1347, -0.2765,  1.2246],\n",
      "         [-0.6293,  0.1016,  0.6211,  ..., -2.5477, -0.3583,  1.2577]],\n",
      "\n",
      "        [[-0.5430,  1.1316, -0.8309,  ..., -0.7445,  0.3259,  0.6411],\n",
      "         [-0.4463, -0.7577,  0.7693,  ..., -0.8927,  2.1681,  0.0897],\n",
      "         [ 0.5419,  0.2254, -1.2923,  ..., -0.3800,  0.6019,  0.0540],\n",
      "         ...,\n",
      "         [ 1.6440,  2.5778,  0.1992,  ...,  0.8075, -0.6799, -1.2590],\n",
      "         [ 1.3132,  0.6200,  1.2461,  ...,  0.8741,  1.6501, -1.8724],\n",
      "         [-1.7863, -0.3020,  0.3158,  ...,  1.5185,  1.2027,  1.1535]]], device='cuda:0')\n",
      "x tensor([[ 0.7180, -0.1102,  0.6078,  0.5961, -0.9205,  0.0180,  0.1004,\n",
      "         -0.6188, -0.0935,  0.2555, -0.0444, -1.1666,  0.8975, -0.3423,\n",
      "          0.6067,  1.0286, -1.1691,  0.3510,  0.5175,  0.9399, -1.7147,\n",
      "         -2.4111, -1.5893, -1.3663,  0.5131,  0.6134,  0.5169, -1.3413,\n",
      "          0.6959,  1.5664],\n",
      "        [-1.8044, -0.8018, -0.8930,  0.5457, -0.5703, -0.3600,  0.1085,\n",
      "          1.2241, -0.1484, -0.6774,  1.4037,  0.5336,  0.2243,  0.3435,\n",
      "          1.6411, -0.3854, -1.5960,  0.1897, -0.7008,  0.5225, -0.6174,\n",
      "          0.5979, -1.3341, -1.2611,  1.7273, -0.2181, -1.5775,  0.4830,\n",
      "         -2.0441, -0.4130],\n",
      "        [ 0.7650,  0.2513,  1.0740, -0.4003, -1.2740, -1.7492, -0.8471,\n",
      "         -1.6916, -0.0791, -0.7998, -0.2803,  0.0941, -1.3391, -0.3437,\n",
      "         -1.8796, -0.7541,  0.9312, -0.5524,  0.6309, -0.1044,  0.7247,\n",
      "          0.1740,  0.8705, -0.2003,  0.5595,  1.1689, -0.1961, -0.5877,\n",
      "         -0.2748, -1.7118],\n",
      "        [-1.6238,  1.0152, -0.1042, -2.8046,  2.4420, -0.9138,  0.9025,\n",
      "         -0.9670, -1.0545,  0.4767, -0.3887,  0.3415,  0.2740,  0.2487,\n",
      "         -0.6490, -0.5686,  0.3732,  1.3893, -0.7816,  0.7673,  1.7266,\n",
      "          0.8258, -0.5997,  0.4527,  0.1811, -0.6192, -0.1270,  0.9139,\n",
      "          0.9297, -0.7429],\n",
      "        [ 0.1268, -0.6967, -1.6286,  0.7682,  0.1337, -0.2550, -2.2256,\n",
      "         -0.3973,  0.9133,  0.7819, -1.0567,  1.7373,  0.7442,  1.1784,\n",
      "          0.3681, -0.3119,  0.0379,  0.2012, -0.1384,  0.9342, -0.6481,\n",
      "          0.0206, -1.4653, -0.8224,  1.4327, -1.2781,  0.4596, -1.3212,\n",
      "         -0.6284, -0.1112],\n",
      "        [ 0.7823, -1.0692,  0.4880,  1.0443, -0.6106, -0.4534, -1.6698,\n",
      "          0.8896, -1.7558,  0.3317,  1.3575,  0.8979, -0.5188, -0.9113,\n",
      "          0.1337, -0.1005, -0.2947,  1.0022, -0.0102,  0.7131, -0.0781,\n",
      "         -0.0368,  0.1392,  0.4968, -0.0478,  0.0959, -0.8730, -0.3368,\n",
      "         -0.1029, -0.2543],\n",
      "        [-0.3512, -0.0339, -1.0063, -1.1439,  0.7125, -2.2349,  1.0208,\n",
      "         -1.6294,  0.2180,  0.8372,  0.5110,  0.3765, -0.1707,  1.0346,\n",
      "         -1.0982,  0.9560, -0.8531, -1.5325, -1.4976, -1.5053,  0.8123,\n",
      "         -0.8910,  0.4239,  0.6544,  0.4839,  0.6051,  0.8503,  0.9497,\n",
      "          1.3851,  0.2517],\n",
      "        [-0.0130, -0.2822,  1.7238,  0.0738, -0.5657, -0.4204,  1.1109,\n",
      "         -0.5277,  1.0509, -1.2052,  1.2247,  1.0224, -0.8561,  1.3440,\n",
      "          0.6656,  0.1836, -0.7385, -0.1491,  0.5773,  1.2472,  0.1257,\n",
      "         -1.1745, -0.2135,  0.8239, -1.8733, -0.0509, -0.9305,  1.4619,\n",
      "         -0.7380, -2.9274],\n",
      "        [ 1.7457, -2.1393, -2.2314, -0.9921,  0.3961, -0.6001,  0.5302,\n",
      "          1.1236, -0.0697,  1.1138,  0.1149,  0.9219,  0.0958,  1.6363,\n",
      "          0.1655,  1.0778,  1.1737, -0.7598, -0.9600,  0.1613,  1.0469,\n",
      "          0.3026,  0.9698,  0.8093,  0.5861,  0.6742,  0.0448, -1.6656,\n",
      "          0.4738,  0.6363],\n",
      "        [-0.6042, -0.2138, -0.9721, -1.0856,  0.1292,  0.2515, -0.3924,\n",
      "         -0.7794,  0.5828,  0.5602,  1.1069,  0.7753, -0.0648, -0.1008,\n",
      "          0.7546, -0.4553, -1.0370, -1.4428, -0.1696, -0.7756,  0.0971,\n",
      "         -0.1771,  0.9783,  0.7088, -1.3509,  0.2222, -0.1969,  0.5648,\n",
      "          1.0080, -0.8419],\n",
      "        [ 0.1997,  0.2573,  1.0465, -0.7076, -0.0806, -1.1907, -2.1385,\n",
      "         -0.5904,  0.8629,  0.9567, -0.0327, -0.6729, -0.2762,  0.2708,\n",
      "         -0.2062,  0.6684, -1.3845,  1.4623,  0.1750, -0.7988,  0.7988,\n",
      "         -0.8756, -0.6124,  1.0082,  0.6081,  0.8207,  1.0889, -0.6401,\n",
      "          0.0107, -0.7110],\n",
      "        [-0.9916,  0.9768, -1.1077, -0.2507,  0.1257,  2.0549, -0.2684,\n",
      "          0.2636,  0.7574,  0.0067,  0.0237,  0.2985,  1.8105, -0.3708,\n",
      "          0.0346, -0.0520, -1.3211, -0.0927, -1.0174,  1.6295,  2.3693,\n",
      "         -0.8212,  0.6811,  0.3955,  0.5868,  0.3346,  0.2398, -1.4313,\n",
      "         -1.0628, -0.1307],\n",
      "        [ 0.2640, -2.2740, -1.4847, -0.1494,  0.7812,  1.5482,  0.6964,\n",
      "          1.1401,  0.2657, -0.8722, -0.4335,  0.5302,  1.1649,  0.9342,\n",
      "         -0.2464,  2.5978, -1.0817, -0.9161,  0.0812, -0.6708,  2.0140,\n",
      "          0.6222, -0.1494, -0.4465,  0.7273,  0.6011, -0.4280,  0.0059,\n",
      "         -0.9537, -0.8619],\n",
      "        [ 1.2618,  0.6207,  0.5026,  1.1269, -1.7056,  0.3991, -0.9498,\n",
      "          1.6139,  0.9936,  0.3019, -0.0434,  0.6432,  1.5282, -0.2452,\n",
      "          0.7952, -1.1224,  0.5172, -0.0711,  0.2193,  0.5887,  0.2547,\n",
      "         -0.0246,  1.0115,  1.8065,  0.9585, -1.5722, -0.2119,  0.2286,\n",
      "         -1.6785, -0.7974],\n",
      "        [-1.2535,  0.3093,  1.2116, -1.0871,  0.3620, -0.7598,  0.4903,\n",
      "         -0.0357, -1.7725,  1.0066,  0.7503, -0.1403,  0.6601,  0.4587,\n",
      "          0.2085, -1.2704, -1.4490,  0.9544,  0.1304,  0.2429,  0.1563,\n",
      "         -0.5269, -0.6908,  0.3678,  0.5994,  0.5080, -0.1557,  1.2666,\n",
      "         -0.6959,  0.8711],\n",
      "        [ 0.1525, -0.2882, -0.2326,  0.4337,  1.4277,  0.3006, -0.3956,\n",
      "         -0.0487,  1.2474,  1.1662,  1.7443,  1.2819,  0.1524,  0.0590,\n",
      "         -0.9749, -0.2889,  0.6443, -0.0201,  0.9018,  0.0015, -1.0263,\n",
      "          0.7618, -0.8880, -1.0609,  0.0889,  0.6406,  0.3648,  0.6416,\n",
      "         -1.5260, -1.2657],\n",
      "        [ 0.6592,  0.0805, -1.1696,  1.2049, -1.5879, -1.1711, -1.1264,\n",
      "          2.0908, -0.6803, -0.7515,  0.4954, -0.6270,  0.6774, -0.2896,\n",
      "         -0.6882, -0.6706,  1.4018,  0.1005,  0.6008,  0.1300,  2.3433,\n",
      "         -0.9907,  0.6397, -1.4172, -0.5396,  0.1214, -0.2950, -0.7129,\n",
      "         -0.7976, -0.0089],\n",
      "        [ 1.2101,  0.1563, -2.5640, -0.3706,  0.4806,  0.0539, -0.4439,\n",
      "         -0.9526,  0.7500,  0.7849, -0.1293, -0.1129,  0.4687,  1.3852,\n",
      "         -0.3887,  0.8667, -0.5216,  1.6788,  0.9556,  0.0594,  0.6656,\n",
      "          0.7624, -0.2365, -1.5753, -0.1923,  0.0079,  0.4480,  0.5213,\n",
      "         -0.4187, -0.2883],\n",
      "        [-0.0119,  0.1858,  0.0913, -0.9669, -0.7635, -0.9445, -0.7105,\n",
      "          0.9788,  0.7424, -1.3202,  0.7936,  0.6186,  0.3462, -1.0429,\n",
      "         -1.0174,  1.2546,  1.1060, -1.0534,  0.5555, -0.7988,  1.2711,\n",
      "          0.0878, -1.5865,  0.3767, -0.0588,  0.5794, -0.2371,  0.9516,\n",
      "         -0.7111, -0.8005],\n",
      "        [ 0.0893, -0.2594, -0.1451,  0.1919,  0.5571, -1.0350, -0.5641,\n",
      "          0.3083,  0.6688,  0.9268,  0.3342,  0.2258,  0.5835,  0.1129,\n",
      "         -0.6294, -0.0127,  0.2660, -0.2374, -0.1179,  2.4424,  2.5099,\n",
      "         -0.3199,  1.3980,  0.4202, -0.7136,  0.5388,  0.6465,  2.0239,\n",
      "         -0.6212,  0.2858]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = Variable(torch.randn(10, 20, 30)).cuda()\n",
    "print(\"x\",x)\n",
    "print(\"x\",x[0,:])\n",
    "lens = np.arange(10) + 1\n",
    "\n",
    "x = pack_padded_sequence(x, lens[::-1], batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0856,  0.0644, -0.3780,  ..., -0.3990, -0.3624,  0.1106],\n",
       "        [-0.7543,  0.2678,  0.9364,  ...,  0.1133,  1.2389, -0.8259],\n",
       "        [ 0.6406,  0.5313,  0.6551,  ...,  1.2418,  0.0284, -0.0325],\n",
       "        ...,\n",
       "        [-0.4823, -0.9832,  0.1907,  ...,  0.5451, -1.3907,  0.8209],\n",
       "        [ 0.0749, -0.5988, -0.1443,  ...,  0.7389,  0.9940, -1.5226],\n",
       "        [ 1.2861, -0.9599, -0.0999,  ...,  0.1092,  1.5392, -0.4489]], device='cuda:0'), batch_sizes=tensor([ 10,   9,   8,   7,   6,   5,   4,   3,   2,   1]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "x = Variable(torch.randn(10, 20, 30)).cuda()\n",
    "lens = np.arange(10) + 1\n",
    "\n",
    "x = pack_padded_sequence(x, lens[::-1], batch_first=True)\n",
    "\n",
    "lstm = nn.LSTM(30, 50, batch_first=True).cuda()\n",
    "h0 = Variable(torch.zeros(1, 10, 50)).cuda()\n",
    "c0 = Variable(torch.zeros(1, 10, 50)).cuda()\n",
    "\n",
    "packed_h, (packed_h_t, packed_c_t) = lstm(x, (h0, c0))\n",
    "h, _ = pad_packed_sequence(packed_h) \n",
    "print (h.size()) # Size 20 x 10 x 50 instead of 10 x 20 x 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal grad..\n",
      "tensor(621.7704, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7701, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7701, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7698, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7693, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7690, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7689, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7684, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7680, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7680, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7676, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7672, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7668, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7666, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7662, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7660, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7657, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7657, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7655, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7648, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7648, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7645, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7642, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7642, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7640, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7635, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7630, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7629, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7625, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7623, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7620, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7617, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7615, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7615, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7609, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7609, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7606, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7603, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7598, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7596, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7595, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7591, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7588, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7586, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7582, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7581, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7578, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7575, device='cuda:0')\n",
      "cal grad..\n",
      "tensor(621.7573, device='cuda:0')\n",
      "cal grad..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a9b66d7baeba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mlast_sent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mtotal_lss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_lss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jongsu/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jongsu/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer1 = optim.SGD(Gru1.parameters(), lr=0.0001)#, weight_decay=1e-4)\n",
    "optimizer2 = optim.SGD(sent_1.parameters(), lr=0.0001)#, weight_decay=1e-4)\n",
    "optimizer3 = optim.SGD(last_sent_1.parameters(), lr=0.0001)#, weight_decay=1e-4)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while(epoch < 100): \n",
    "    \n",
    "    total_lss = 0\n",
    "    dialogue_idx = 1\n",
    "    while(dialogue_idx =< len(my_dataset)/100 ):\n",
    "        \n",
    "        if(dialogue_idx == 1):\n",
    "            batch_dialogue = my_dataset[0:100]\n",
    "        else:\n",
    "            batch_dialogue = my_dataset[dialogue_idx-100:dialogue_idx] #make batch\n",
    "            \n",
    "            \n",
    "        sent_idx = 0\n",
    "        new_sent_to_vec = []\n",
    "        sent_to_vec = torch.tensor(new_sent_to_vec).cuda()\n",
    "        \n",
    "        \n",
    "        make_label = []\n",
    "        new_label = np.array([])\n",
    "        while(sent_idx < len(per_dialogue['dialogue'][0]) -1): #indexed dialogue\n",
    "            \n",
    "            ## sent_lstm training\n",
    "            sent = per_dialogue['dialogue'][0][sent_idx]\n",
    "            sent_len = len(per_dialogue['dialogue'][0][sent_idx]) \n",
    "            \n",
    "            word_idx = 0\n",
    "            vec = []\n",
    "            \n",
    "            \n",
    "            while(word_idx < sent_len): \n",
    "                \n",
    "                \n",
    "                encoder_hidden = chr_rnn_1.initHidden()\n",
    "                if per_dialogue['dialogue'][1][sent_idx][word_idx] == 1: #word_to_vec\n",
    "                    word_to_tensor = torch.tensor(wv_model_en.wv[wv_model_en.wv.index2entity[sent[word_idx]]])\n",
    "                    gpu_wt = word_to_tensor.cuda()\n",
    "                    vec.append(gpu_wt) #append known word vec\n",
    "                else: #character model\n",
    "                    for ei in range(len(sent[word_idx])):\n",
    "                        gru_input = torch.tensor(np.zeros(shape = (1, 1, 100), dtype=\"float32\")).cuda()\n",
    "                        gru_input[0][0][ord(sent[word_idx][ei])%100] = 1 #character to onehot vec\n",
    "                        encoder_hidden = chr_rnn_1(gru_input,encoder_hidden)\n",
    "\n",
    "                    last_encoder_hidden = encoder_hidden.view(-1,200)\n",
    "                    last_encoder_hidden_ = last_net_1(last_encoder_hidden) #last hiddenstate and fully c l\n",
    "                    vec.append(last_encoder_hidden_) #append unknown word vec\n",
    "                word_idx = word_idx + 1\n",
    "            \n",
    "            \n",
    "            word_idx = 0 \n",
    "            \n",
    "            sent_hidden = sent_1.initHidden()\n",
    "            \n",
    "            \n",
    "            while(word_idx<sent_len):\n",
    "\n",
    "                view = vec[word_idx].view(1,1,100)\n",
    "                sent_hidden = sent_1(view,sent_hidden)\n",
    "                word_idx = word_idx + 1\n",
    "            \n",
    "            # 200 -> 100\n",
    "            \n",
    "            sent_vec = last_sent_1(sent_hidden.view(-1,200))\n",
    "   \n",
    "            sent_to_vec = torch.cat((sent_to_vec,sent_vec),0)\n",
    "            \n",
    "\n",
    "            index1_toint = int(per_dialogue['label'][0][sent_idx*2]) *4\n",
    "            index2_toint = int(per_dialogue['label'][1][sent_idx*2])\n",
    "            \n",
    "        \n",
    "            new_label = np.append(new_label,index1_toint + index2_toint -1) #hot first index,, index1 prob\n",
    "            \n",
    "            sent_idx = sent_idx + 1\n",
    "        \n",
    "        \n",
    "        resh_stv = sent_to_vec.view(-1,1,100).cuda()\n",
    "        \n",
    "        torch_make_label = torch.LongTensor(new_label).cuda()\n",
    "\n",
    "        resh_stv = Gru1(resh_stv)\n",
    "        \n",
    "        loss = Gru1.Loss(resh_stv, torch_make_label) #-> \n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "        ## lstm_crf training\n",
    "\n",
    "        dialogue_idx = dialogue_idx + 1\n",
    "        \n",
    "        total_lss = total_lss + loss\n",
    "        \n",
    "        if(dialogue_idx % 1 ==0):\n",
    "            print(\"cal grad..\")\n",
    "            Gru1.zero_grad()\n",
    "            sent_1.zero_grad()\n",
    "            last_sent_1.zero_grad()\n",
    "            \n",
    "            total_lss.backward()\n",
    "            print(total_lss)\n",
    "            \n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            optimizer3.step()\n",
    "            \n",
    "            total_lss = 0\n",
    "            \n",
    "            torch.save(Gru1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/Bigru_2.ckpt')\n",
    "            torch.save(sent_1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/sent_2.ckpt')\n",
    "            torch.save(last_sent_1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/last_sent_2.ckpt')\n",
    "\n",
    "            torch.save(Gru1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/Bigru_2.pkl')\n",
    "            torch.save(sent_1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/sent_2.pkl')\n",
    "            torch.save(last_sent_1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/last_sent_2.pkl')\n",
    "\n",
    "            torch.save(Gru1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/Bigru_2.pth')\n",
    "            torch.save(sent_1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/sent_2.pth')\n",
    "            torch.save(last_sent_1.state_dict(),'/home/jongsu/Desktop/intention_extraction/crf_lstm_parameter/last_sent_2.pth')\n",
    "        \n",
    "    epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
