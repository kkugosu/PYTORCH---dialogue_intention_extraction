{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import io,os\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tag_to_ix = {'start_tag':0,'stop_tag':29,'pad_tag':30}\n",
    "\n",
    "working_path = '/home/jongsu/jupyter/pytorch_dialogue_ie/'\n",
    "WV_PATH = '/home/jongsu/jupyter/pytorch_dialogue_ie/parameter/dialogue_wv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def sent_loader(sentence): #pre_process per sentence\n",
    "    result = []\n",
    "    for elem in sentence.split(' '):\n",
    "        if elem != '':\n",
    "            result = np.append(result, elem)\n",
    "    return result,len(result)\n",
    "\n",
    "def cal_dialogue(data):\n",
    "    i = 0\n",
    "    maxleng = 0\n",
    "    while(i < len(data.Text)):\n",
    "        if len(data.Text[i]) > maxleng:\n",
    "            maxleng = len(data.Text[i])\n",
    "        i = i + 1\n",
    "    return maxleng\n",
    "\n",
    "def batchload(dataset, repeat, BATCH_SIZE, a):\n",
    "\n",
    "    #dataset = sorted(dataset, key = lambda  x: cal_dialogue(x))\n",
    "    #a = torch.ones(len(dataset)-1000,dtype=torch.int).fill_(2004)\n",
    "    #a = torch.arange(end = len(dataset)-1000,dtype=torch.int)\n",
    "    while True:\n",
    "        i = BATCH_SIZE\n",
    "        while(i <= len(a)):\n",
    "            batch = []\n",
    "            batch_seq = 0\n",
    "            batchnum = a[i-BATCH_SIZE:i]\n",
    "            \n",
    "            while(batch_seq < BATCH_SIZE):\n",
    "                batch.append(dataset[batchnum[batch_seq]])\n",
    "                batch_seq = batch_seq + 1\n",
    "            print(i)\n",
    "            yield batch\n",
    "            i = i + BATCH_SIZE\n",
    "        \n",
    "        if repeat == False:\n",
    "            break\n",
    "\n",
    "    \n",
    "def pad_batch(minibatch):\n",
    "    i = 0\n",
    "    new_batch = []\n",
    "    leng_set = []\n",
    "    maxleng = 1\n",
    "    sentnum_per_dialogue = []\n",
    "    while(i < len(minibatch)): #almost equal to BATCH_SIZE\n",
    "        j = 0\n",
    "        temp = []\n",
    "        sentnum_per_dialogue.append(len(minibatch[i].Text)) #stoptag\n",
    "        #save sentnum per dialogue\n",
    "        while(j < len(minibatch[i].Text)-1):\n",
    "            sent, leng = sent_loader(minibatch[i].Text[j])  \n",
    "            \n",
    "            #convert text to word_list, word_list_length\n",
    "            leng_set.append(leng)\n",
    "            if leng > maxleng:\n",
    "                maxleng = leng\n",
    "\n",
    "            temp.append(sent)\n",
    "            j = j + 1\n",
    "        \n",
    "        temp.append([\"<stop_tag>\"])\n",
    "        leng_set.append(1) #stoptag\n",
    "        new_batch.append(temp)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    i = 0\n",
    "    while (i < len(minibatch)): #almost equal to BATCH_SIZE\n",
    "        j = 0\n",
    "        while (j < len(new_batch[i])):\n",
    "\n",
    "            while(len(new_batch[i][j]) < maxleng):\n",
    "                new_batch[i][j] = np.append(new_batch[i][j],\"<pad>\")\n",
    "\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    #batch * sentnum * (word_list+pad) -> new_batch\n",
    "    #batch * sentnum * (word_list_length) -> leng_set\n",
    "    #batch * (sentnum) -> sentnum_per_dialogue\n",
    "\n",
    "    return new_batch, leng_set ,sentnum_per_dialogue\n",
    "\n",
    "\n",
    "wv_model = word2vec.Word2Vec(size = 100, window = 5, min_count = 5, workers = 4)\n",
    "wv_model = word2vec.Word2Vec.load(WV_PATH)\n",
    "\n",
    "def numerize_sent(sent, len_sent):\n",
    "    i = 0\n",
    "    n_sent = []\n",
    "    while(i < len_sent):\n",
    "        if(sent[i] == '<pad>'):\n",
    "            n_sent.append(np.zeros(100))\n",
    "            \n",
    "        elif(sent[i] == '<stop_tag>'):\n",
    "            n_sent.append(np.ones(100))\n",
    "        else:\n",
    "            try:\n",
    "                n_sent.append(wv_model.wv[sent[i]])\n",
    "            except:\n",
    "                n_sent.append(np.zeros(100))\n",
    "\n",
    "        i = i + 1\n",
    "    return n_sent\n",
    "\n",
    "def batch_numerical(sent_set):\n",
    "    numeric_batch = []#numerized batch\n",
    "    i = 0\n",
    "    while(i < len(sent_set) ): #BATCH_SIZE\n",
    "        dial = []#numerized dialogue\n",
    "        j = 0\n",
    "        while(j < len(sent_set[i])): #per dialogue\n",
    "            '''\n",
    "            sent_set[i][j] ['Is' 'this' 'your' 'new' 'teacher' '?' '<pad>']\n",
    "            sent_set[i][j] ['Yes' ',' 'it' 'is' '.' '<pad>' '<pad>']\n",
    "            sent_set[i][j] ['Is' 'she' 'short' '?' '<pad>' '<pad>' '<pad>']\n",
    "            sent_set[i][j] ['No' ',' 'she' '’' 's' 'average' '.']\n",
    "            sent_set[i][j] ['What' 'color' 'are' 'her' 'eyes' '?' '<pad>']\n",
    "            sent_set[i][j] ['They' '’' 're' 'dark' 'gray' '.' '<pad>']\n",
    "            sent_set[i][j] ['What' 'color' 'is' 'her' 'hair' '?' '<pad>']\n",
    "            sent_set[i][j] ['It' '’' 's' 'blond' '.' '<pad>' '<pad>']\n",
    "            sent_set[i][j] ['And' 'how' 'old' 'is' 'she' '?' '<pad>']\n",
    "            sent_set[i][j] ['I' 'don' '’' 't' 'know' '.' '<pad>']\n",
    "            sent_set[i][j] ['<stop_tag>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>' '<pad>']\n",
    "            '''\n",
    "            dial.append(numerize_sent(sent_set[i][j], len(sent_set[i][j]))) #numerized_sentence\n",
    "            j = j + 1\n",
    "        numeric_batch.append(dial)\n",
    "        i = i + 1\n",
    "    #batch * sent_num * sent_leng * wv -> numeric_batch\n",
    "    return numeric_batch\n",
    "\n",
    "def make_batch2sent(new):\n",
    "    for_sentmodel = []\n",
    "    batchnum = 0\n",
    "    while (batchnum < len(new)): #BATCH_SIZE\n",
    "        for_sentmodel = for_sentmodel + new[batchnum]\n",
    "        batchnum = batchnum + 1\n",
    "    sentbatch_len = len(for_sentmodel)\n",
    "    #batch * sent_num * sent_leng * wv -> all_sent_num * sent_leng * wv\n",
    "    return sentbatch_len, for_sentmodel\n",
    "\n",
    "def pad_dial(last_v):\n",
    "    leng_set = []\n",
    "    i = 0\n",
    "    while(i < len(last_v)):#BATCH_SIZE\n",
    "        leng_set.append(len(last_v[i]))#sentence num\n",
    "        i = i + 1\n",
    "    padded_dial = pad_sequence(last_v, batch_first = True)#append padtag vector\n",
    "    #print('max_dialogue_length',len(padded_dial[0]))\n",
    "    \n",
    "    return padded_dial, leng_set\n",
    "\n",
    "def sent_loader(sentence): #pre_process per sentence\n",
    "    result = []\n",
    "    for elem in sentence.split(' '):\n",
    "        if elem != '':\n",
    "            result = np.append(result, elem)\n",
    "    return result,len(result)\n",
    "\n",
    "def pad_cat_tag(emotion, act): \n",
    "    i = 0\n",
    "    new_tag = []\n",
    "    while(i < len(emotion)): #BATCH_SIZE\n",
    "        emo, lenge = sent_loader(emotion[i][0])\n",
    "        ac, lenga = sent_loader(act[i][0])\n",
    "        j = 0\n",
    "        inte = []\n",
    "        inte.append(tag_to_ix['start_tag']) #append stop tag\n",
    "        while(j < len(emo)): #sent length\n",
    "            inte.append(int(emo[j]) * 4 + int(ac[j]))\n",
    "            j = j + 1\n",
    "        inte.append(tag_to_ix['stop_tag']) #append stop tag\n",
    "        torch_inte = torch.tensor(inte)\n",
    "        new_tag.append(torch_inte) #str to int\n",
    "        i = i + 1\n",
    "\n",
    "            \n",
    "    padded_tag = pad_sequence(new_tag, batch_first = True, padding_value = tag_to_ix['pad_tag'])\n",
    "    \n",
    "    #emotion+action string -> emotion+action numb + padding\n",
    "    #batch*tag\n",
    "    return padded_tag\n",
    "\n",
    "def make_mask(leng):\n",
    "    '''\n",
    "    make one-hot vector of mask from lengset\n",
    "    '''\n",
    "\n",
    "    var = np.zeros(shape = (len(leng), leng[0])) #len(leng) = BATCH_SIZE, leng[0]+1= largest dialogue + stop\n",
    "    i = 0\n",
    "    while(i < len(leng)):#BATCH_SIZE\n",
    "        j = 0\n",
    "        while(j < leng[0]): \n",
    "            if(j < leng[i]): # <= stop tag\n",
    "                var[i][j] = 1\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "    return var\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    max_score, _ = torch.max(x, -1)\n",
    "    max_score_broadcast = max_score.unsqueeze(-1).expand_as(x)\n",
    "    return max_score + torch.log(torch.sum(torch.exp(x - max_score_broadcast), -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class for load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    @classmethod\n",
    "    def fromdict(cls, data, fields):\n",
    "        ex = cls()\n",
    "        for key, vals in fields.items():\n",
    "            if key not in data:\n",
    "                raise ValueError(\"Specified key {} was not found in \"\n",
    "                                 \"the input data\".format(key))\n",
    "            if vals is not None:\n",
    "                if not isinstance(vals, list):\n",
    "                    vals = [vals]\n",
    "                for val in vals:\n",
    "                    name, field = val\n",
    "                    setattr(ex, name, field.preprocess(data[key]))\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    sort_key = None\n",
    "\n",
    "    def __init__(self, examples, fields, filter_pred=None):\n",
    "        self.examples = examples\n",
    "\n",
    "        self.fields = dict(fields)\n",
    "\n",
    "        # Unpack field tuples\n",
    "        for n, f in list(self.fields.items()):\n",
    "            if isinstance(n, tuple):\n",
    "                self.fields.update(zip(n, f))\n",
    "                del self.fields[n]\n",
    "        self.pp = tuple(d for d in self.examples if d is not None)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, path=None, root='.data', train=None, **kwargs):\n",
    "\n",
    "        train_data = cls(os.path.join(path, train), **kwargs)\n",
    "        #print(train_data.examples) #여기엔 field example둘다 들어있음\n",
    "        #print(tuple(d for d in train_data if d is not None)) #여기엔 example만 나열된 튜플이됨\n",
    "        return tuple(d for d in train_data if d is not None)\n",
    "\n",
    "    def split(self, split_ratio=0.7, stratified=False, strata_field='label',\n",
    "              random_state=None):\n",
    "        \n",
    "        train_ratio, test_ratio, val_ratio = check_split_ratio(split_ratio)\n",
    "\n",
    "        # For the permutations\n",
    "        rnd = RandomShuffler(random_state)\n",
    "        if not stratified:\n",
    "            train_data, test_data, val_data = rationed_split(self.examples, train_ratio,\n",
    "                                                             test_ratio, val_ratio, rnd)\n",
    "        else:\n",
    "            if strata_field not in self.fields:\n",
    "                raise ValueError(\"Invalid field name for strata_field {}\"\n",
    "                                 .format(strata_field))\n",
    "            strata = stratify(self.examples, strata_field)\n",
    "            train_data, test_data, val_data = [], [], []\n",
    "            for group in strata:\n",
    "                # Stratify each group and add together the indices.\n",
    "                group_train, group_test, group_val = rationed_split(group, train_ratio,\n",
    "                                                                    test_ratio, val_ratio,\n",
    "                                                                    rnd)\n",
    "                train_data += group_train\n",
    "                test_data += group_test\n",
    "                val_data += group_val\n",
    "\n",
    "        splits = tuple(Dataset(d, self.fields)\n",
    "                       for d in (train_data, val_data, test_data) if d)\n",
    "\n",
    "        # In case the parent sort key isn't none\n",
    "        if self.sort_key:\n",
    "            for subset in splits:\n",
    "                subset.sort_key = self.sort_key\n",
    "        return splits\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        try:\n",
    "            return len(self.examples)\n",
    "        except TypeError:\n",
    "            return 2 ** 32\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.examples:\n",
    "            yield x\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.fields:\n",
    "            for x in self.examples:\n",
    "                yield getattr(x, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class my_TabularDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path,  fields,  **kwargs):\n",
    "\n",
    "\n",
    "        with open(path, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                examples = [Example.fromdict(per_data, my_fields) for per_data in json.loads(line)]\n",
    "\n",
    "\n",
    "        if isinstance(fields, dict):\n",
    "            fields, field_dict = [], fields\n",
    "            for field in field_dict.values():\n",
    "                if isinstance(field, list):\n",
    "                    fields.extend(field)\n",
    "                else:\n",
    "                    fields.append(field)\n",
    "\n",
    "\n",
    "        super(my_TabularDataset, self).__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_1 (shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class makesent_gru(nn.Module):\n",
    "    def __init__(self, hidden_size, bidirectional):\n",
    "        super(makesent_gru, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if (bidirectional == True):\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.gru = nn.GRU(100, 100, bidirectional=bidirectional)\n",
    "        self.lastnet = nn.Linear(200, 100)\n",
    "        \n",
    "    def masking_f(self, new_sent, all_seq_len):\n",
    "        remake = torch.transpose(new_sent, 0, 1)\n",
    "        #remake [326, 7, 200]\n",
    "        \n",
    "        i = 0\n",
    "        while(i < len(remake)):\n",
    "            if i == 0:\n",
    "                # all_seq_len[0]-1 -> 5\n",
    "                tensor = remake[0][all_seq_len[0]-1].view(1,200)\n",
    "            else:\n",
    "                tensor = torch.cat((tensor, remake[i][all_seq_len[i]-1].view(1,200)), 0)\n",
    "            i = i + 1\n",
    "        return tensor\n",
    "    \n",
    "    def forward(self, char, h0, masking_v):\n",
    "        #char 7,326,100 [6][0] = 0000....<pad>\n",
    "        gru_out, h0 = self.gru(char, h0)\n",
    "        #gru 7,326,200\n",
    "        \n",
    "        last_hidden_state = self.masking_f(gru_out, masking_v)\n",
    "        #[326,200]\n",
    "        \n",
    "        last_w = self.lastnet(last_hidden_state)\n",
    "        #[326,100]\n",
    "        \n",
    "        return last_w\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.bidirectional, 1, self.hidden_size, device=device, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_2 \n",
    "\n",
    "## compare crf-gru with crf , bi-gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CRF_(nn.Module):\n",
    "    def __init__(self, tag_to_ix, hidden_dim):\n",
    "        super(BiGru_CRF, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(100, 100, bidirectional=True) # default requires_grad = true\n",
    "        self.hidden2tag = nn.Linear(100, 31) # default requires_grad = true\n",
    "        self.transitions = nn.Parameter(torch.randn(31, 31))# [a,b] trans from b to a,  requires_grad = true\n",
    "        self.transitions.data[0, :] = -10000 #all to start\n",
    "        self.transitions.data[:, 29] = -10000 #stop to all\n",
    "        self.transitions.data[:, 30] = -10000 #pad to all\n",
    "        self.transitions.data[30][30] = 0 #pad to pad\n",
    "        self.transitions.data[29][30] = 0 #stop to pad\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        \n",
    "\n",
    "    def init_hidden(self,batch):\n",
    "        return Variable(torch.zeros(2, batch, 100).cuda()) # default requires_grad = false \n",
    "    \n",
    "    def _get_gru_features(self, batch, sentence_set):\n",
    "        \n",
    "        hidden = self.init_hidden(batch)\n",
    "        #gru_out, hidden = self.gru(sentence_set, hidden)\n",
    "\n",
    "        gru_feats = self.hidden2tag(sentence_set)\n",
    "\n",
    "        return gru_feats\n",
    "\n",
    "    def for_score(self, pre_mask, feats):\n",
    "        score = Variable(torch.zeros((BATCH_SIZE, 31)).fill_(-10000.).cuda()) #default requires_grad = false \n",
    "        score[:, self.tag_to_ix['start_tag']] = 0. #start to all is 0\n",
    "        \n",
    "        mask = Variable(torch.Tensor(pre_mask).cuda()) # default requires_grad = false \n",
    "        \n",
    "        for t in range(feats.size(0)):  # 안에서 연산하는데이터들은 batch*featuresize*featuresize\n",
    "            \n",
    "            mask_t = mask[:, t].unsqueeze(-1).expand_as(score) #batch_size -> batch_size*featuresize\n",
    "            \n",
    "            score_t = score.unsqueeze(1).expand(-1, *self.transitions.size()) #batch_size*f -> batch_size*f*f\n",
    "            \n",
    "            emit = feats[t].unsqueeze(-1).expand_as(score_t) #b*f-> b*f*f\n",
    "            \n",
    "            trans = self.transitions.unsqueeze(0).expand_as(score_t) #b*f*f\n",
    "            \n",
    "            score_t = log_sum_exp(score_t + emit + trans)\n",
    "            \n",
    "            score = score_t * mask_t + score * (1 - mask_t) #no updating in masked score,all b*f\n",
    "\n",
    "        score = log_sum_exp(score)\n",
    "        return score\n",
    "\n",
    "    def cal_score(self, mask, feats, tag):\n",
    "        score = Variable(torch.FloatTensor(BATCH_SIZE).fill_(0.).cuda()) # default requires_grad = false \n",
    "        \n",
    "        temp_tag = Variable(tag.cuda()) # default requires_grad = false \n",
    "        mask_tensor = torch.transpose(torch.FloatTensor(mask), 0, 1) #seq*batch\n",
    "        mask_tensor = Variable(mask_tensor.cuda()) # default requires_grad = false \n",
    "        \n",
    "        \n",
    "        for i, feat in enumerate(feats): #seq*batch*feat->batch*feat\n",
    "            \n",
    "            transit = torch.cat(\n",
    "                [torch.tensor([self.transitions[temp_tag[batch][i + 1], temp_tag[batch][i]]]) for batch in range(BATCH_SIZE)])\n",
    "            \n",
    "            transit = transit.cuda()\n",
    "            \n",
    "            transit = transit * mask_tensor[i] #batch*batch->batch\n",
    "\n",
    "            emit = torch.cat([feat[batch][temp_tag[batch][i + 1]].view(1, -1) for batch in range(BATCH_SIZE)]).squeeze(1)\n",
    "\n",
    "            emit = emit * mask_tensor[i]#batch*batch->batch\n",
    "\n",
    "            score = score + transit + emit\n",
    "\n",
    "        return score\n",
    "\n",
    "    def neg_log_likelihood(self, mask, sentence, tags, batch):\n",
    "\n",
    "        feats = self._get_gru_features(batch, sentence)\n",
    "        \n",
    "        forward_score = self.for_score(mask, feats)\n",
    "\n",
    "        gold_score = self.cal_score(mask, feats, tags)\n",
    "        '''\n",
    "        newt = self.transitions.data.cpu().numpy()\n",
    "        newt[0,:] = 0\n",
    "        newt[:,29] = 0\n",
    "        newt[:,30] = 0\n",
    "        x = np.tile(np.arange(1, 32), (31, 1))\n",
    "        y = x.transpose()\n",
    "        z = newt #for visdom\n",
    "        print(z)\n",
    "        \n",
    "        \n",
    "        x = np.tile(np.arange(1, 32), (31, 1))\n",
    "        y = x.transpose()\n",
    "        z = (x + y)/20\n",
    "        \n",
    "        # surface\n",
    "        viz.surf(X=z, opts=dict(colormap='Hot'))\n",
    "        '''\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def _viterbi_decode(self, mask, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = Variable(torch.full((1, 31), -10000.).cuda()) # default requires_grad = false \n",
    "        init_vvars[0][0] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        for i, feat in enumerate(feats):\n",
    "            if mask[i] == 0:\n",
    "                print('breaked')\n",
    "                break\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "            \n",
    "            #if i < (len(feats)-1):\n",
    "            for next_tag in range(31):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            #else:\n",
    "            #    next_tag_var = forward_var + self.transitions[29]\n",
    "            #    best_tag_id = argmax(next_tag_var)\n",
    "            #    bptrs_t.append(best_tag_id)\n",
    "            #    viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        #terminal_var = forward_var  + self.transitions[self.tag_to_ix['stop_tag']]\n",
    "        best_tag_id = argmax(forward_var) #not terminal_var\n",
    "        path_score = forward_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == 0  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def forward(self,batch, dummy_input, seq):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_gru_features(batch,dummy_input[1])\n",
    "        \n",
    "        lstm_feats = torch.transpose(lstm_feats,0,1)\n",
    "        mask = dummy_input[0]\n",
    "        #print(self.transitions)\n",
    "        # Find the best path, given thue features.\n",
    "        score, tag_seq = self._viterbi_decode(mask[seq], lstm_feats[seq])\n",
    "        return score, tag_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiGru_(nn.Module):\n",
    "    def __init__(self, tag_to_ix, hidden_dim):\n",
    "        super(BiGru_CRF, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(100, 100, bidirectional=True) # default requires_grad = true\n",
    "        self.hidden2tag = nn.Linear(200, 31) # default requires_grad = true\n",
    " \n",
    "\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        \n",
    "    def _get_gru_features(self, batch, sentence_set):\n",
    "        \n",
    "        hidden = self.init_hidden(batch)\n",
    "        gru_out, hidden = self.gru(sentence_set, hidden)\n",
    "\n",
    "        gru_feats = self.hidden2tag(gru_out)\n",
    "\n",
    "        return gru_feats\n",
    "\n",
    "\n",
    "    def forward(self,batch, dummy_input, seq):  # dont confuse this with _forward_alg above.\n",
    "        feats = self._get_gru_features(batch, sentence)\n",
    "        \n",
    "      \n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiGru_CRF(nn.Module):\n",
    "    def __init__(self, tag_to_ix, hidden_dim):\n",
    "        super(BiGru_CRF, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(100, 100, bidirectional=True) # default requires_grad = true\n",
    "        self.hidden2tag = nn.Linear(200, 31) # default requires_grad = true\n",
    "        self.transitions = nn.Parameter(torch.randn(31, 31))# [a,b] trans from b to a,  requires_grad = true\n",
    "        self.transitions.data[0, :] = -10000 #all to start\n",
    "        self.transitions.data[:, 29] = -10000 #stop to all\n",
    "        self.transitions.data[:, 30] = -10000 #pad to all\n",
    "        self.transitions.data[30][30] = 0 #pad to pad\n",
    "        self.transitions.data[29][30] = 0 #stop to pad\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        \n",
    "\n",
    "    def init_hidden(self,batch):\n",
    "        return Variable(torch.zeros(2, batch, 100).cuda()) # default requires_grad = false \n",
    "    \n",
    "    def _get_gru_features(self, batch, sentence_set):\n",
    "        \n",
    "        hidden = self.init_hidden(batch)\n",
    "        gru_out, hidden = self.gru(sentence_set, hidden)\n",
    "\n",
    "        gru_feats = self.hidden2tag(gru_out)\n",
    "\n",
    "        return gru_feats\n",
    "\n",
    "    def for_score(self, pre_mask, feats):\n",
    "        score = Variable(torch.zeros((BATCH_SIZE, 31)).fill_(-10000.).cuda()) #default requires_grad = false \n",
    "        score[:, self.tag_to_ix['start_tag']] = 0. #start to all is 0\n",
    "        \n",
    "        mask = Variable(torch.Tensor(pre_mask).cuda()) # default requires_grad = false \n",
    "        \n",
    "        for t in range(feats.size(0)):  # 안에서 연산하는데이터들은 batch*featuresize*featuresize\n",
    "            \n",
    "            mask_t = mask[:, t].unsqueeze(-1).expand_as(score) #batch_size -> batch_size*featuresize\n",
    "            \n",
    "            score_t = score.unsqueeze(1).expand(-1, *self.transitions.size()) #batch_size*f -> batch_size*f*f\n",
    "            \n",
    "            emit = feats[t].unsqueeze(-1).expand_as(score_t) #b*f-> b*f*f\n",
    "            \n",
    "            trans = self.transitions.unsqueeze(0).expand_as(score_t) #b*f*f\n",
    "            \n",
    "            score_t = log_sum_exp(score_t + emit + trans)\n",
    "            \n",
    "            score = score_t * mask_t + score * (1 - mask_t) #no updating in masked score,all b*f\n",
    "\n",
    "        score = log_sum_exp(score)\n",
    "        return score\n",
    "\n",
    "    def cal_score(self, mask, feats, tag):\n",
    "        score = Variable(torch.FloatTensor(BATCH_SIZE).fill_(0.).cuda()) # default requires_grad = false \n",
    "        \n",
    "        temp_tag = Variable(tag.cuda()) # default requires_grad = false \n",
    "        mask_tensor = torch.transpose(torch.FloatTensor(mask), 0, 1) #seq*batch\n",
    "        mask_tensor = Variable(mask_tensor.cuda()) # default requires_grad = false \n",
    "        \n",
    "        \n",
    "        for i, feat in enumerate(feats): #seq*batch*feat->batch*feat\n",
    "            \n",
    "            transit = torch.cat(\n",
    "                [torch.tensor([self.transitions[temp_tag[batch][i + 1], temp_tag[batch][i]]]) for batch in range(BATCH_SIZE)])\n",
    "            \n",
    "            transit = transit.cuda()\n",
    "            \n",
    "            transit = transit * mask_tensor[i] #batch*batch->batch\n",
    "\n",
    "            emit = torch.cat([feat[batch][temp_tag[batch][i + 1]].view(1, -1) for batch in range(BATCH_SIZE)]).squeeze(1)\n",
    "\n",
    "            emit = emit * mask_tensor[i]#batch*batch->batch\n",
    "\n",
    "            score = score + transit + emit\n",
    "\n",
    "        return score\n",
    "\n",
    "    def neg_log_likelihood(self, mask, sentence, tags, batch):\n",
    "\n",
    "        feats = self._get_gru_features(batch, sentence)\n",
    "        \n",
    "        forward_score = self.for_score(mask, feats)\n",
    "\n",
    "        gold_score = self.cal_score(mask, feats, tags)\n",
    "        '''\n",
    "        newt = self.transitions.data.cpu().numpy()\n",
    "        newt[0,:] = 0\n",
    "        newt[:,29] = 0\n",
    "        newt[:,30] = 0\n",
    "        x = np.tile(np.arange(1, 32), (31, 1))\n",
    "        y = x.transpose()\n",
    "        z = newt #for visdom\n",
    "        print(z)\n",
    "        \n",
    "        \n",
    "        x = np.tile(np.arange(1, 32), (31, 1))\n",
    "        y = x.transpose()\n",
    "        z = (x + y)/20\n",
    "        \n",
    "        # surface\n",
    "        viz.surf(X=z, opts=dict(colormap='Hot'))\n",
    "        '''\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def _viterbi_decode(self, mask, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = Variable(torch.full((1, 31), -10000.).cuda()) # default requires_grad = false \n",
    "        init_vvars[0][0] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        for i, feat in enumerate(feats):\n",
    "            if mask[i] == 0:\n",
    "                print('breaked')\n",
    "                break\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "            \n",
    "            #if i < (len(feats)-1):\n",
    "            for next_tag in range(31):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            #else:\n",
    "            #    next_tag_var = forward_var + self.transitions[29]\n",
    "            #    best_tag_id = argmax(next_tag_var)\n",
    "            #    bptrs_t.append(best_tag_id)\n",
    "            #    viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        #terminal_var = forward_var  + self.transitions[self.tag_to_ix['stop_tag']]\n",
    "        best_tag_id = argmax(forward_var) #not terminal_var\n",
    "        path_score = forward_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == 0  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def forward(self,batch, dummy_input, seq):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_gru_features(batch,dummy_input[1])\n",
    "        \n",
    "        lstm_feats = torch.transpose(lstm_feats,0,1)\n",
    "        mask = dummy_input[0]\n",
    "        #print(self.transitions)\n",
    "        # Find the best path, given thue features.\n",
    "        score, tag_seq = self._viterbi_decode(mask[seq], lstm_feats[seq])\n",
    "        return score, tag_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_preprocess(sent, batch_data):\n",
    "\n",
    "    \n",
    "    #sorted with dialogue length\n",
    "    #print(batch_data[0].Text)\n",
    "    #######################################################\n",
    "    emotion_set = []\n",
    "    action_set = []\n",
    "    i = 0\n",
    "    while(i < len(batch_data)): #equals to BATCH_SIZE except last dataset\n",
    "        emotion_set.append(batch_data[i].labels_1)\n",
    "        \n",
    "        action_set.append(batch_data[i].labels_2)\n",
    "        \n",
    "        i = i + 1\n",
    "    \n",
    "    new_tag = pad_cat_tag(emotion_set, action_set)\n",
    "    '''\n",
    "    new_tag representation 0 = start_tag, 29 = stop_tag, 30 = pad_tag\n",
    "    \n",
    "    tensor([  0,   2,   1,   2,   1,   2,   1,   2,   1,   2,   1,  29])\n",
    "    tensor([  0,   2,   1,   2,   1,   2,   1,   2,   1,   2,   1,  29])\n",
    "    tensor([  0,  21,   1,   1,   1,  29,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,   2,   1,   2,   1,  29,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,   1,   1,   1,   1,  29,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,   2,   1,   2,   1,  29,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,  17,  17,  17,  29,  30,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,   1,   1,   1,  29,  30,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,   3,   2,  29,  30,  30,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,  17,  17,  29,  30,  30,  30,  30,  30,  30,  30,  30])\n",
    "    tensor([  0,   2,   2,  29,  30,  30,  30,  30,  30,  30,  30,  30])\n",
    "    '''\n",
    "    #batch*tag\n",
    "    new_tag = Variable(new_tag.cuda())# default requires_grad = false\n",
    "    \n",
    "    #####################################################tag_preprocess\n",
    "    \n",
    "    \n",
    "    new, all_seq_len, sentnum_per_batch = pad_batch(batch_data)\n",
    "    #batch * sentnum * (word_list+pad) -> new\n",
    "    #batch * sentnum * (word_list_length) -> all_seq_len\n",
    "    #batch * (sentnum) -> sentnum_per_batch\n",
    "    '''\n",
    "    new[0]\n",
    "    [array(['Is', 'this', 'your', 'new', 'teacher', '?', '<pad>'], dtype='<U32'), \n",
    "      array(['Yes', ',', 'it', 'is', '.', '<pad>', '<pad>'], dtype='<U32'), \n",
    "      array(['Is', 'she', 'short', '?', '<pad>', '<pad>', '<pad>'], dtype='<U32'), \n",
    "      array(['No', ',', 'she', '’', 's', 'average', '.'], dtype='<U32'), \n",
    "      array(['What', 'color', 'are', 'her', 'eyes', '?', '<pad>'], dtype='<U32'), \n",
    "      array(['They', '’', 're', 'dark', 'gray', '.', '<pad>'], dtype='<U32'), \n",
    "      array(['What', 'color', 'is', 'her', 'hair', '?', '<pad>'], dtype='<U32'),\n",
    "      array(['It', '’', 's', 'blond', '.', '<pad>', '<pad>'], dtype='<U32'), \n",
    "      array(['And', 'how', 'old', 'is', 'she', '?', '<pad>'], dtype='<U32'), \n",
    "      array(['I', 'don', '’', 't', 'know', '.', '<pad>'], dtype='<U32'), \n",
    "      array(['<stop_tag>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
    "      \n",
    "    all_seq_len[0]\n",
    "    [6, 5, 4, 7, 6, 6, 6, 5, 6, 6, 1, \n",
    "    6, 5, 4, 7, 6, 6, 6, 5, 6, 6, 1, \n",
    "    6, 5, 7, 2, 1, \n",
    "    5, 6, 5, 6, 1, \n",
    "    4, 4, 6, 4, 1, \n",
    "    6, 5, 5, 5, 1, \n",
    "    4, 4, 3, 1, \n",
    "    5, 5, 4, 1, \n",
    "    3, 3, 1, \n",
    "    3, 2, 1, \n",
    "    3, 2, 1, \n",
    "    3, 3, 1, \n",
    "    4, 2, 1, \n",
    "    4, 2, 1, \n",
    "    3, 3, 1, \n",
    "    4, 3, 1, \n",
    "    4, 3, 1, \n",
    "    4, 3, 1, \n",
    "    4, 3, 1, \n",
    "    3, 3, 1, \n",
    "    4, 2, 1, \n",
    "    3, 3, 1, \n",
    "    5, 4, 1, \n",
    "    4, 3, 1, \n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    sentnum_per_batch\n",
    "    [11, 11, 5, 5, 5, 5, 4, 4, 3, 3, 3, 3, .....]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    new2 = batch_numerical(new)\n",
    "    #batch * sent_num * sent_leng * wv -> new2\n",
    "\n",
    "    \n",
    "    sentbatch_len, for_sentmodel = make_batch2sent(new2)\n",
    "    #batch * sent_num * sent_leng * wv -> all_sent_num(new_batch) * sent_leng * wv\n",
    "    #for_sentmodel2 -> torch.Size([326, 7, 100])\n",
    "    #sentbatch_len -> 326\n",
    "    \n",
    "    hidden_state = torch.tensor(np.zeros((2, sentbatch_len, 100)), dtype=torch.float, device= device, requires_grad=False)\n",
    "    for_sentmodel2 = torch.tensor(np.transpose(for_sentmodel, [1, 0, 2]), dtype=torch.float, device= device, requires_grad=False)\n",
    "    \n",
    "    #for_sentmodel2 -> torch.Size([7, 326, 100])\n",
    "    \n",
    "    pre_crf_gru = sent(for_sentmodel2, hidden_state, all_seq_len) \n",
    "    '''\n",
    "    print(pre_crf_gru[0])\n",
    "    print(pre_crf_gru[1])\n",
    "    print(pre_crf_gru[2])\n",
    "    print(pre_crf_gru[3])\n",
    "    print(pre_crf_gru[4])\n",
    "    print(pre_crf_gru[5])\n",
    "    print(pre_crf_gru[6])\n",
    "    print(pre_crf_gru[7])\n",
    "    print(pre_crf_gru[8])\n",
    "    print(pre_crf_gru[9])\n",
    "    '''\n",
    "    #pre_crf_gru -> torch.Size([326, 100])\n",
    "    \n",
    "    #################################################sent network\n",
    "    #all_sent_num(new_batch) * sent_leng * wv -> all_sent_num(new_batch) * wv\n",
    "    \n",
    "    \n",
    "    \n",
    "    last_var = torch.split(pre_crf_gru, sentnum_per_batch)\n",
    "    #all_sent_num(new_batch) * wv -> batch * sent_num * wv\n",
    "    '''\n",
    "    last_var\n",
    "    \n",
    "    [11,100]\n",
    "    [11,100]\n",
    "    [5,100]\n",
    "    [5,100]\n",
    "    [5,100]\n",
    "    .\n",
    "    .\n",
    "    '''\n",
    "    new_dial, dial_leng = pad_dial(last_var)\n",
    "    #batch * sent_num * wv -> batch * (sent_num + pad) * wv\n",
    "    #save dial_leng for masking\n",
    "    \n",
    "    '''\n",
    "    new_dial\n",
    "    \n",
    "    [11,100]\n",
    "    [11,100]\n",
    "    [11,100]\n",
    "    [11,100]\n",
    "    [11,100]\n",
    "    .\n",
    "    .\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_dial = torch.transpose(new_dial, 0, 1)\n",
    "    \n",
    "    \n",
    "\n",
    "    return new_dial, new_tag, dial_leng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "sent_to_vextor_bigru_net = makesent_gru(100, True).cuda()\n",
    "#we need convert batch*dialogue -> batch*(dialogue_length*sent_vec(float))\n",
    "\n",
    "my_grucrf_model = BiGru_CRF(tag_to_ix,BATCH_SIZE).cuda()\n",
    "#this convert batch*(dialogue_length*sent_vec(float)) -> batch*(dialogue_tag,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_models(path1,path2):\n",
    "    sent_to_vextor_bigru_net.load_state_dict(torch.load(path1))\n",
    "    my_grucrf_model.load_state_dict(torch.load(path2))\n",
    "    return\n",
    "\n",
    "load_models('./shared.pth','./crf_gru.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "my_fields={'dial': ('Text', data.Field(sequential=True)),\n",
    "        'emo': ('labels_1', data.Field(sequential=False)),\n",
    "        'act': ('labels_2', data.Field(sequential=False))}\n",
    "\n",
    "train = my_TabularDataset.splits(path = working_path, train = 'full_data.json',\n",
    "                          fields=my_fields) \n",
    "train = sorted(train, key = lambda  x: cal_dialogue(x))\n",
    "train = train[:-1118]\n",
    "train = sorted(train, key = lambda  x: -len(x.Text))\n",
    "print(len(train))\n",
    "\n",
    "optimizer1 = optim.SGD(sent_to_vextor_bigru_net.parameters(), lr=0.000003, weight_decay=1e-4)\n",
    "optimizer2 = optim.SGD(my_grucrf_model.parameters(), lr=0.000003, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rep1(arr,k,newary,batchnum):\n",
    "    i = 0\n",
    "    err_count = 0\n",
    "    while i < len(arr):\n",
    "        if arr[i] < k:\n",
    "            arr[i] = 0\n",
    "        elif arr[i] > (k*4):\n",
    "            err_count = err_count + 1\n",
    "            newary.append(i + batchnum*100)\n",
    "            #print(i,arr[i])\n",
    "        i = i + 1\n",
    "    print (\"###############################################errcount\",err_count)\n",
    "    return arr, newary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_func(train, sent_to_vextor_bigru_net, my_grucrf_model, dataseq, filtering_value, iter_num):\n",
    "    newary_ = []\n",
    "    k = 0\n",
    "    for batch_data in batchload(train, repeat=True, BATCH_SIZE = BATCH_SIZE, a = dataseq ):\n",
    "        #load txt data from jsonfile\n",
    "\n",
    "        sent_to_vextor_bigru_net.zero_grad()\n",
    "        my_grucrf_model.zero_grad()\n",
    "\n",
    "        new_dial, new_tag, dial_leng = all_preprocess(sent_to_vextor_bigru_net, batch_data)\n",
    "        #load batch*(dialogue_length*sent_vec(float)) -> new_dial\n",
    "        #load batch*tag -> new_tag\n",
    "        #load batch * dial_leng\n",
    "\n",
    "        loss = my_grucrf_model.neg_log_likelihood(make_mask(dial_leng), new_dial, new_tag, BATCH_SIZE)\n",
    "        #print(loss)\n",
    "        loss,newary_ = rep1(loss,filtering_value, newary_,k,batchsize = BATCH_SIZE)\n",
    "        batch_loss = torch.sum(loss)\n",
    "        #print(batch_loss)\n",
    "        batch_loss.backward(retain_graph=False)\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "        unuselist = [new_dial, new_tag, dial_leng]\n",
    "        del unuselist\n",
    "\n",
    "        k = k + 1\n",
    "        print(k)\n",
    "        if k%10 == 0:\n",
    "            torch.save(sent_to_vextor_bigru_net.state_dict(),'./shared.pth')\n",
    "            torch.save(my_grucrf_model.state_dict(),'./crf_gru.pth') #3.53 save with dummy\n",
    "            dummy_input = [make_mask(dial_leng),new_dial]\n",
    "            print(new_tag[7])\n",
    "            print(my_grucrf_model(BATCH_SIZE,dummy_input,seq=7))\n",
    "            print('################')\n",
    "            print(new_tag[15])\n",
    "            print(my_grucrf_model(BATCH_SIZE,dummy_input,seq=15))\n",
    "            print('################')\n",
    "            print(new_tag[23])\n",
    "            print(my_grucrf_model(BATCH_SIZE,dummy_input,seq=23))\n",
    "            print('################')\n",
    "            print(new_tag[31])\n",
    "            print(my_grucrf_model(BATCH_SIZE,dummy_input,seq=31))\n",
    "            print('################')\n",
    "            print(new_tag[39])\n",
    "            print(my_grucrf_model(BATCH_SIZE,dummy_input,seq=39))\n",
    "            print(loss)\n",
    "        if k == int(len(train)/BATCH_SIZE)*iter_num:\n",
    "            break\n",
    "            \n",
    "        if k % int(len(train)/BATCH_SIZE) == 0:\n",
    "            newary = newary_\n",
    "            newary_ = []\n",
    "    print(newary)\n",
    "    \n",
    "    return newary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "###############################################errcount 31\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524580978845/work/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-473a7fd9d2af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merrary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_to_vextor_bigru_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_grucrf_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f7975614b13a>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(train, sent_to_vextor_bigru_net, my_grucrf_model, dataseq, filtering_value, iter_num)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(batch_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jongsu/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jongsu/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524580978845/work/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "errary = []\n",
    "dataseq = torch.arange(end = len(train),dtype=torch.int)\n",
    "\n",
    "errary = train_func(train, sent_to_vextor_bigru_net, my_grucrf_model,dataseq, 10, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataseq = torch.tensor(errary, dtype=torch.int)\n",
    "\n",
    "newary = train_func(train, sent_to_vextor_bigru_net, my_grucrf_model, dataseq, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-4.2.0]",
   "language": "python",
   "name": "conda-env-anaconda3-4.2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
